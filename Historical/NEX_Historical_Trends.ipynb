{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPItGliLCBB8W8LVoALPimj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"id":"_baFEsIV37JW","executionInfo":{"status":"ok","timestamp":1714203539723,"user_tz":420,"elapsed":6,"user":{"displayName":"Andrew Fullhart","userId":"10918403777587533818"}}},"outputs":[],"source":["%reset -f"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import ee\n","ee.Authenticate()\n"],"metadata":{"id":"Wq5xL57nZdFM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714203545661,"user_tz":420,"elapsed":1203,"user":{"displayName":"Andrew Fullhart","userId":"10918403777587533818"}},"outputId":"dbc53a58-61c5-44e6-dfe3-737588660159"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import os\n","import ee\n","\n","ee.Initialize(project='ee-andrewfullhart')\n","\n","ic = ee.ImageCollection('NASA/NEX-GDDP')\n","\n","inFILE = '/content/drive/My Drive/Colab Notebooks/Script Input Files/GHCN_Historical_Annual_Results.csv'\n","\n","study_area = ee.FeatureCollection('users/andrewfullhart/SW_Study_Area')\n","\n","ndays_months = ee.List([31., 28.25, 31., 30., 31., 30., 31., 31., 30., 31., 30., 31.])\n","order_months = ee.List([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n","\n","\n","\"\"\"Append nothing to output file name if running this block.\"\"\"\n","\n","with open(inFILE) as f:\n","  lines = f.readlines()\n","\n","points_ft_list = ee.List([])\n","points_label_list = ee.List([])\n","for line in lines[1:]:\n","  row = line.strip('\\n').split(',')\n","  stationID = ee.String(row[0])\n","  lon = float(row[1])\n","  lat = float(row[2])\n","  ft = ee.Feature(None, {'stationID':stationID}).setGeometry(ee.Geometry.Point(lon, lat))\n","  points_ft_list = points_ft_list.add(ft)\n","  points_label_list = points_label_list.add(stationID)\n","\n","points_fc = ee.FeatureCollection(points_ft_list)\n","\n","\n","#\"\"\"Append _CLINET to output file name if running this block.\"\"\"\n","\n","#points_ft_list = ee.List([])\n","#points_label_list = ee.List([])\n","#parFOLDER = '/content/drive/My Drive/Colab Notebooks/CLIGEN/2015parfiles/2015parfiles/2015parfiles'\n","#par_files = os.listdir(parFOLDER)\n","#for parf in par_files:\n","#  if parf[:2] in ['az', 'nv', 'nm', 'ut', 'US']:\n","#    stationID = parf.strip('.par')\n","#    if stationID[0] == 'z':\n","#       stationID = 'a' + stationID\n","#    with open(os.path.join(parFOLDER, parf)) as f:\n","#      lines = f.readlines()\n","#    for line in lines:\n","#      label = line[:8]\n","#      label = label.rstrip().lstrip()\n","#      dataline = line[8:]\n","#      if label == 'LATT=':\n","#        lat = float(line.split('=')[1].strip('LONG').strip())\n","#        lon = float(line.split('=')[2].strip('YEARS').strip())\n","#    ft = ee.Feature(None, {}).setGeometry(ee.Geometry.Point(lon, lat))\n","#    points_ft_list = points_ft_list.add(ft)\n","#    points_label_list = points_label_list.add(stationID)\n","\n","#points_fc = ee.FeatureCollection(points_ft_list)\n","\n","\n","start_year_global = 1974\n","end_year_global = 2013\n","years_list_global = ee.List.sequence(start_year_global, end_year_global)\n","years_index_global = ee.List.sequence(0, years_list_global.size().subtract(1))\n","\n","def pointlabel_fn(point):\n","  point = ee.Feature(point).geometry()\n","  a_strng = ee.Number(point.coordinates().get(0)).format('%.3f')\n","  b_strng = ee.String('_')\n","  c_strng = ee.Number(point.coordinates().get(1)).format('%.3f')\n","  point_strng = a_strng.cat(b_strng).cat(c_strng)\n","  point_strng = point_strng.replace('\\\\.', '_', 'g').slice(1)\n","  return point_strng\n","\n","point_labels_global = points_fc.toList(points_fc.size()).map(pointlabel_fn)\n","\n","start = ee.Date.fromYMD(ee.Number(start_year_global), 1, 1)\n","end = ee.Date.fromYMD(ee.Number(end_year_global).add(1), 1, 1)\n","year_ic = ic.filterDate(start, end).select('pr')\n","modelfilter = ee.Filter.Or(\n","              ee.Filter.eq('scenario', 'historical'),\n","              ee.Filter.eq('scenario', 'rcp45'))\n","year_ic = year_ic.filter(modelfilter)\n","year_ic = year_ic.filter(ee.Filter.eq('model', 'CCSM4'))\n","\n","def point_fn(point):\n","  stationID = ee.String(point.get('stationID'))\n","  point = point.geometry()\n","  data_list = year_ic.getRegion(point, 500).slice(1)\n","\n","  def feature_fn(l):\n","    l = ee.List(l)\n","    date_str = ee.String(ee.String(l.get(0)).split('_').get(-1))\n","    year = ee.Number(date_str.slice(0, 4))\n","    month = ee.Number(date_str.slice(4, 6))\n","    prcp = ee.Number(l.get(4))\n","    prop_dict = {'year':year, 'month':month, 'precip':prcp}\n","    point_ft = ee.Feature(None, prop_dict)\n","    return point_ft\n","\n","  data_fc = ee.FeatureCollection(data_list.map(feature_fn))\n","  nonzero_fc = data_fc.filter(ee.Filter.gt('precip', 0))\n","  number_avg = ee.Number(nonzero_fc.reduceColumns(ee.Reducer.mean(), ['precip']).get('mean')).multiply(86400)\n","  number_avg = ee.Number(ee.Algorithms.If(nonzero_fc.size().gt(0), number_avg, 0))\n","  number_sdv = ee.Number(nonzero_fc.reduceColumns(ee.Reducer.stdDev(), ['precip']).get('stdDev')).multiply(86400)\n","  number_sdv = ee.Number(ee.Algorithms.If(nonzero_fc.size().gt(0), number_sdv, 0))\n","  number_skw = ee.Number(nonzero_fc.reduceColumns(ee.Reducer.skew(), ['precip']).get('skew'))\n","  number_skw = ee.Number(ee.Algorithms.If(number_sdv.eq(0), 0, number_skw))\n","  #number_skw = ee.Number(ee.Algorithms.If(number_sdv.eq(1), 0, number_skw))\n","  number_skw = ee.Number(ee.Algorithms.If(nonzero_fc.size().gt(0), number_skw, 0))\n","  number_ft = ee.Feature(None, {'point':stationID,'mean':number_avg, 'sdev':number_sdv, 'skew':number_skw})\n","  return number_ft\n","\n","out_fc_agg = ee.FeatureCollection(points_fc.map(point_fn))\n","\n","task_agg = ee.batch.Export.table.toDrive(collection=out_fc_agg,\n","                           description='NEX_Historical_Annual_Trends_Agg',\n","                           folder='GEE_Downloads')\n","\n","task_agg.start()\n","\n","\n","\n","def year_fn(year):\n","  year = ee.Number(year)\n","  start = ee.Date.fromYMD(ee.Number(year), 1, 1)\n","  end = ee.Date.fromYMD(ee.Number(year).add(1), 1, 1)\n","  year_ic = ic.filterDate(start, end).select('pr')\n","  modelfilter = ee.Filter.Or(\n","                ee.Filter.eq('scenario', 'historical'),\n","                ee.Filter.eq('scenario', 'rcp45'))\n","  year_ic = year_ic.filter(modelfilter)\n","  year_ic = year_ic.filter(ee.Filter.eq('model', 'CCSM4'))\n","\n","\n","  def point_fn(point):\n","    point = point.geometry()\n","    a_strng = ee.Number(point.coordinates().get(0)).format('%.3f')\n","    b_strng = ee.String('_')\n","    c_strng = ee.Number(point.coordinates().get(1)).format('%.3f')\n","    point_strng = a_strng.cat(b_strng).cat(c_strng)\n","    point_strng = point_strng.replace('\\\\.', '_', 'g').slice(1)\n","    data_list = year_ic.getRegion(point, 500).slice(1)\n","\n","    def feature_fn(l):\n","      l = ee.List(l)\n","      date_str = ee.String(ee.String(l.get(0)).split('_').get(-1))\n","      year = ee.Number(date_str.slice(0, 4))\n","      month = ee.Number(date_str.slice(4, 6))\n","      prcp = ee.Number(l.get(4))\n","      prop_dict = {'year':year, 'month':month, 'precip':prcp}\n","      point_ft = ee.Feature(None, prop_dict)\n","      return point_ft\n","\n","    data_fc = ee.FeatureCollection(data_list.map(feature_fn))\n","    nonzero_fc = data_fc.filter(ee.Filter.gt('precip', 0))\n","    number_avg = ee.Number(nonzero_fc.reduceColumns(ee.Reducer.mean(), ['precip']).get('mean')).multiply(86400)\n","    number_avg = ee.Number(ee.Algorithms.If(nonzero_fc.size().gt(0), number_avg, 0))\n","    number_sdv = ee.Number(nonzero_fc.reduceColumns(ee.Reducer.stdDev(), ['precip']).get('stdDev')).multiply(86400)\n","    number_sdv = ee.Number(ee.Algorithms.If(nonzero_fc.size().gt(0), number_sdv, 0))\n","    number_skw = ee.Number(nonzero_fc.reduceColumns(ee.Reducer.skew(), ['precip']).get('skew'))\n","    number_skw = ee.Number(ee.Algorithms.If(number_sdv.eq(0), 0, number_skw))\n","    #number_skw = ee.Number(ee.Algorithms.If(number_sdv.eq(1), 0, number_skw))\n","    number_skw = ee.Number(ee.Algorithms.If(nonzero_fc.size().gt(0), number_skw, 0))\n","    number_ft = ee.Feature(None, {'point':stationID,'mean':number_avg, 'sdev':number_sdv, 'skew':number_skw})\n","    return number_ft\n","\n","  means_fc = ee.FeatureCollection(points_fc.map(point_fn))\n","  return means_fc\n","\n","means_fc_list = ee.FeatureCollection(years_list_global.map(year_fn)).toList(years_list_global.size())\n","\n","def flatten_fn(mean_fc):\n","  mean_fc = ee.FeatureCollection(mean_fc)\n","  data_list_mean = ee.List(mean_fc.reduceColumns(ee.Reducer.toList(), ['mean']).get('list'))\n","  data_list_sdev = ee.List(mean_fc.reduceColumns(ee.Reducer.toList(), ['sdev']).get('list'))\n","  data_list_skew = ee.List(mean_fc.reduceColumns(ee.Reducer.toList(), ['skew']).get('list'))\n","  prop_dict_mean = ee.Dictionary(points_label_list.zip(data_list_mean).flatten())\n","  prop_dict_sdev = ee.Dictionary(points_label_list.zip(data_list_sdev).flatten())\n","  prop_dict_skew = ee.Dictionary(points_label_list.zip(data_list_skew).flatten())\n","  return  ee.Feature(None, {'mean':prop_dict_mean, 'sdev':prop_dict_sdev, 'skew':prop_dict_skew})\n","\n","out_fc = ee.FeatureCollection(means_fc_list.map(flatten_fn))\n","\n","task = ee.batch.Export.table.toDrive(collection=out_fc,\n","                           description='NEX_Historical_Annual_Trends',\n","                           folder='GEE_Downloads')\n","\n","task.start()\n","\n","\n"],"metadata":{"id":"z7ZjSx2IFgda","executionInfo":{"status":"ok","timestamp":1714199184791,"user_tz":420,"elapsed":1653,"user":{"displayName":"Andrew Fullhart","userId":"10918403777587533818"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import statsmodels.api as sm\n","import numpy as np\n","import re\n","\n","geeresFILE = '/content/drive/My Drive/GEE_Downloads/NEX_Historical_Annual_Trends_Agg.csv'\n","geedataFILE = '/content/drive/My Drive/GEE_Downloads/NEX_Historical_Annual_Trends.csv'\n","resFILE = '/content/drive/My Drive/Colab Notebooks/NEX_Historical_Annual_Trends_Results.csv'\n","dataFILE = '/content/drive/My Drive/Colab Notebooks/NEX_Historical_Annual_Trends_Data.csv'\n","\n","\"\"\"For GHCN blocks\"\"\"\n","inFILE = '/content/drive/My Drive/Colab Notebooks/Script Input Files/GHCN_Historical_Annual_Results.csv'\n","\"\"\"For _CLINET blocks\"\"\"\n","#inFILE = '/content/drive/My Drive/Colab Notebooks/Script Input Files/US_CLIGEN_Coords.csv'\n","\n","with open(inFILE) as f:\n","  lines = f.readlines()\n","\n","stationIDjoin_Dict = {}\n","for line in lines[1:]:\n","  row = line.strip('\\n').split(',')\n","  lon = row[1]\n","  lat = row[2]\n","  stationID = row[0]\n","  stationIDjoin_Dict[stationID] = [lon, lat]\n","\n","with open(geeresFILE) as f:\n","  lines = f.readlines()\n","\n","agg_data = {}\n","for line in lines[1:]:\n","  row = line.split(',')\n","  stationID = row[2]\n","  mean_agg = row[1]\n","  sdev_agg = row[3]\n","  skew_agg = row[4]\n","  agg_data[stationID] = [mean_agg, sdev_agg, skew_agg]\n","\n","with open(geedataFILE) as f:\n","  lines = f.readlines()\n","\n","data_dict = {'mean':{iD:[] for iD in stationIDjoin_Dict}, 'sdev':{iD:[] for iD in stationIDjoin_Dict}, 'skew':{iD:[] for iD in stationIDjoin_Dict}}\n","for line in lines[1:]:\n","  stat_dict_row = re.findall('{' + '(.+?)' + '}', line)\n","  for i, s in enumerate(['mean', 'sdev', 'skew']):\n","    row = stat_dict_row[i].strip('\\n').split(',')\n","    for item in row:\n","      pair = item.strip().split('=')\n","      stationID = pair[0]\n","      number = pair[1]\n","      data_dict[s][stationID].append(number)\n","\n","write_data = []\n","\n","for iD in stationIDjoin_Dict:\n","  lon = stationIDjoin_Dict[iD][0]\n","  lat = stationIDjoin_Dict[iD][1]\n","  slopes = []\n","  for i, s in enumerate(['mean', 'sdev', 'skew']):\n","    y = [float(number) for number in data_dict[s][iD]]\n","    x = np.arange(0, 40)\n","    X = sm.add_constant(x)\n","    model = sm.OLS(y, X)\n","    results = model.fit()\n","    slope = results.params[1]\n","    slopes.append(slope)\n","\n","  slope_means = str(slopes[0])\n","  slope_sdevs = str(slopes[1])\n","  slope_skews = str(slopes[2])\n","  mean = agg_data[iD][0]\n","  sdev = agg_data[iD][1]\n","  skew = agg_data[iD][2]\n","\n","  write_data.append([iD, lon, lat, slope_means, mean, slope_sdevs, sdev, slope_skews, skew])\n","\n","\n","with open(resFILE, 'w') as fo:\n","  fo.write('stationID,x,y,slope_means,mean,slope_sdevs,sdev,slope_skews,skew\\n')\n","  for row in write_data:\n","    fo.write(','.join(row) + '\\n')\n","\n","with open(dataFILE, 'w') as fo:\n","  hdr = ''\n","  for iD in stationIDjoin_Dict:\n","    hdr += ',{}_means,{}_sdevs,{}_skews'.format(iD, iD, iD)\n","  fo.write('year' + hdr + '\\n')\n","\n","  for i, yr in enumerate(list(range(1974, 2014))):\n","    data_str = str(yr)\n","    for iD in stationIDjoin_Dict:\n","      for s in ['mean', 'sdev', 'skew']:\n","        number_str = data_dict[s][iD][i]\n","        data_str = data_str + ',' + number_str\n","    data_str += '\\n'\n","    fo.write(data_str)\n","\n","\n","\n"],"metadata":{"id":"ouSkfHOwkOfF","executionInfo":{"status":"ok","timestamp":1714203561595,"user_tz":420,"elapsed":1099,"user":{"displayName":"Andrew Fullhart","userId":"10918403777587533818"}}},"execution_count":9,"outputs":[]}]}